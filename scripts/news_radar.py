import os
import sys
import yfinance as yf
import requests
import datetime
import feedparser
import urllib.parse
import warnings

# å¿½ç•¥ yfinance è­¦å‘Š
warnings.filterwarnings("ignore")

# =============================
# 1. åŸºç¤èˆ‡ç’°å¢ƒè¨­å®š
# =============================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

DISCORD_WEBHOOK_URL = os.getenv("NEWS_WEBHOOK_URL", "").strip()
CACHE_FILE = os.path.join(DATA_DIR, "news_cache.txt")
TZ_TW = datetime.timezone(datetime.timedelta(hours=8))

MAX_EMBEDS = 8
NEWS_HOURS_LIMIT = 12
PRICE_CACHE = {}

# =============================
# 2. è‚¡åƒ¹èˆ‡æŒ‡æ•¸ç²å–ç³»çµ±
# =============================
def get_stock_price(sym):
    if sym in PRICE_CACHE: return PRICE_CACHE[sym]
    try:
        t = yf.Ticker(sym)
        info = t.fast_info
        price = info.get("last_price") or t.info.get("regularMarketPrice")
        prev = info.get("previous_close") or t.info.get("regularMarketPreviousClose")
        if price and prev:
            pct = ((price - prev) / prev) * 100
            PRICE_CACHE[sym] = (price, pct)
            return price, pct
    except: pass
    PRICE_CACHE[sym] = (None, None)
    return None, None

def get_market_price(market_type):
    try:
        sym = "^TWII" if market_type == "TW" else "^IXIC"
        name = "åŠ æ¬ŠæŒ‡æ•¸" if market_type == "TW" else "é‚£æ–¯é”å…‹"
        t = yf.Ticker(sym)
        info = t.fast_info
        cur = info.get("last_price") or t.info.get("regularMarketPrice")
        prev = info.get("previous_close") or t.info.get("regularMarketPreviousClose")
        if not cur or not prev: return "âš ï¸ è³‡æ–™è®€å–ä¸­"
        pct = ((cur - prev) / prev) * 100
        emoji = "ğŸ“ˆ" if pct > 0 else "ğŸ“‰" if pct < 0 else "â–"
        return f"{emoji} {name}: {cur:.2f} ({pct:+.2f}%)"
    except: return "âš ï¸ æŒ‡æ•¸å–å¾—å¤±æ•—"

# =============================
# 3. å€‹è‚¡å°ç…§è¡¨ (AI æ ¸å¿ƒæ¨™çš„)
# =============================
STOCK_MAP = {
    "å°ç©é›»": {"sym": "2330.TW", "desc": "AIæ™¶ç‰‡ / å…ˆé€²è£½ç¨‹"},
    "2330": {"sym": "2330.TW", "desc": "AIæ™¶ç‰‡ / å…ˆé€²è£½ç¨‹"},
    "é´»æµ·": {"sym": "2317.TW", "desc": "AIä¼ºæœå™¨ / çµ„è£"},
    "è¯ç™¼ç§‘": {"sym": "2454.TW", "desc": "ICè¨­è¨ˆ"},
    "å»£é”": {"sym": "2382.TW", "desc": "AIä¼ºæœå™¨ä»£å·¥"},
    "å¥‡é‹": {"sym": "3017.TW", "desc": "AIæ•£ç†±é¾é ­"},
    "00929": {"sym": "00929.TW", "desc": "ç§‘æŠ€å„ªæ¯"},
    "00919": {"sym": "00919.TW", "desc": "ç²¾é¸é«˜æ¯"},
    "è¼é”": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "NVIDIA": {"sym": "NVDA", "desc": "NVIDIA / AIé¾é ­"},
    "ç‰¹æ–¯æ‹‰": {"sym": "TSLA", "desc": "Tesla"},
    "TSLA": {"sym": "TSLA", "desc": "Tesla"},
    "è˜‹æœ": {"sym": "AAPL", "desc": "Apple"},
    "AAPL": {"sym": "AAPL", "desc": "Apple"},
    "å¾®è»Ÿ": {"sym": "MSFT", "desc": "Microsoft"},
    "PLTR": {"sym": "PLTR", "desc": "AIæ•¸æ“šåˆ†æ"},
}

STOCK_WEIGHT = {"2330.TW": 5, "NVDA": 5, "AAPL": 4, "2454.TW": 4, "PLTR": 3}

def pick_most_important_stock(title):
    hits = []
    title_lower = title.lower()
    seen_sym = set()
    for key, info in STOCK_MAP.items():
        if key.lower() in title_lower:
            sym = info["sym"]
            if sym in seen_sym: continue
            seen_sym.add(sym)
            weight = STOCK_WEIGHT.get(sym, 1)
            # åˆ†æ•¸ = æ¬Šé‡ * 100 - å‡ºç¾ä½ç½® (è¶Šå‰é¢è¶Šé‡è¦)
            hits.append((weight * 100 - title_lower.find(key.lower()), info))
    if not hits: return None
    return sorted(hits, reverse=True)[0][1]

# =============================
# 4. Discord è¨Šæ¯ç”Ÿæˆ
# =============================
def create_news_embed(post, market_type):
    color = 0x3498db if market_type == "TW" else 0xe74c3c
    target = pick_most_important_stock(post["title"])

    if target:
        price, pct = get_stock_price(target["sym"])
        if price is not None:
            trend = "ğŸ“ˆ åˆ©å¤š" if pct > 0 else "ğŸ“‰ åˆ©ç©º" if pct < 0 else "â– ä¸­æ€§"
            return {
                "title": f"ğŸ“Š {target['sym']} | {target['desc']}",
                "url": post["link"],
                "color": color,
                "fields": [
                    {"name": "âš–ï¸ å¸‚å ´åˆ¤æ–·", "value": trend, "inline": True},
                    {"name": "ğŸ’µ å³æ™‚åƒ¹æ ¼", "value": f"**{price:.2f} ({pct:+.2f}%)**", "inline": True},
                    {"name": "ğŸ“° ç„¦é»æ–°è", "value": f"[{post['title']}]({post['link']})\nğŸ•’ {post['time']}", "inline": False},
                ],
                "footer": {"text": "Quant Master Radar"}
            }
    
    return {
        "title": post["title"],
        "url": post["link"],
        "color": color,
        "fields": [
            {"name": "ğŸ•’ ç™¼å¸ƒæ™‚é–“", "value": f"{post['time']} (å°åŒ—)", "inline": True},
            {"name": "ğŸ“° æ–°èä¾†æº", "value": post["source"], "inline": True},
        ],
        "footer": {"text": "Quant Master Radar"}
    }

# =============================
# 5. ä¸»æµç¨‹é‚è¼¯
# =============================
def run_radar():
    if not DISCORD_WEBHOOK_URL:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š NEWS_WEBHOOK_URL"); return

    # è®€å–å¿«å–
    sent_titles = set()
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            sent_titles = {l.strip() for l in f if l.strip()}

    now_tw = datetime.datetime.now(TZ_TW)
    market_type = "TW" if 7 <= now_tw.hour < 16 else "US"
    
    queries = (["å°è‚¡ è²¡ç¶“", "å°ç©é›» é´»æµ· è¯ç™¼ç§‘"] if market_type == "TW" 
               else ["ç¾è‚¡ ç›¤å‰", "è¼é” NVIDIA ç‰¹æ–¯æ‹‰", "PLTR è²¡å ±"])

    now_utc = datetime.datetime.now(datetime.timezone.utc)
    collected = {}

    for q in queries:
        url = f"https://news.google.com/rss/search?q={urllib.parse.quote(q)}&hl=zh-TW&gl=TW&ceid=TW:zh-TW"
        feed = feedparser.parse(url)
        for e in feed.entries[:10]:
            title = e.title.split(" - ")[0]
            if title in sent_titles or title in collected: continue
            if not hasattr(e, "published_parsed"): continue
            pub_utc = datetime.datetime(*e.published_parsed[:6], tzinfo=datetime.timezone.utc)
            if (now_utc - pub_utc).total_seconds() / 3600 > NEWS_HOURS_LIMIT: continue
            
            collected[title] = {
                "title": title, "link": e.link, "source": e.title.split(" - ")[-1],
                "time": pub_utc.astimezone(TZ_TW).strftime("%H:%M"), "sort": pub_utc,
            }

    posts = sorted(collected.values(), key=lambda x: x["sort"], reverse=True)[:MAX_EMBEDS]
    if not posts: return

    embeds = [create_news_embed(p, market_type) for p in posts]
    
    # ç™¼é€è‡³ Discord
    market_label = "ğŸ¹ å°è‚¡å³æ™‚é›·é”" if market_type == "TW" else "âš¡ ç¾è‚¡å³æ™‚é›·é”"
    payload = {
        "content": f"## {market_label}\nğŸ“Š **{get_market_price(market_type)}**\nğŸ“… å°åŒ—æ™‚é–“: `{now_tw.strftime('%Y-%m-%d %H:%M')}`\n{'-'*25}",
        "embeds": embeds
    }
    
    try:
        r = requests.post(DISCORD_WEBHOOK_URL, json=payload, timeout=15)
        if r.status_code in (200, 204):
            # æˆåŠŸå¾Œæ›´æ–°å¿«å– (ä¿ç•™æœ€æ–° 300 æ¢)
            new_cache = (list(sent_titles) + [p["title"] for p in posts])[-300:]
            with open(CACHE_FILE, "w", encoding="utf-8") as f:
                for t in new_cache: f.write(f"{t}\n")
            print(f"âœ… æ¨æ’­æˆåŠŸ: {len(posts)} å‰‡")
    except Exception as e:
        print(f"âŒ æ¨æ’­å¤±æ•—: {e}")

if __name__ == "__main__":
    run_radar()
